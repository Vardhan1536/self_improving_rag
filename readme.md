# ðŸ§  Self-Improving Retrieval-Augmented Generation (RAG) System

> A production-oriented RAG system that **detects its own failures**, **adapts retrieval and prompting strategies**, and **learns from past mistakes** to improve reliability over time.

---

## ðŸ” Why This Project Exists

Most RAG systems stop at:

> *â€œUpload documents â†’ retrieve top-k â†’ generate answerâ€*

This approach fails silently in production due to:

* poor chunking
* partial retrieval coverage
* hallucinations
* overconfident answers on insufficient context

**This project addresses those failures directly.**

Instead of optimizing for â€œaccuracyâ€, it optimizes for:

* **faithfulness**
* **grounding**
* **confidence calibration**
* **system-level learning**

---

## âœ¨ Key Capabilities

### âœ… Multi-Strategy Ingestion & Chunking

* Fixed / sliding window chunking
* Section-aware chunking
* Semantic (sentence-based) chunking
* Strategy metadata preserved per chunk

### âœ… Retrieval with Quality Introspection

* Dense vector retrieval (FAISS + sentence-transformers)
* Retrieval diagnostics:

  * semantic similarity
  * query coverage
  * redundancy

### âœ… GPU-Accelerated Generation

* HuggingFace causal LLMs
* Automatic GPU/CPU offloading
* 4-bit NF4 quantization (BitsAndBytes)
* Memory-aware inference

### âœ… LLM Evaluation (Not Accuracy-Based)

* **Faithfulness**: Is the answer supported by retrieved context?
* **Grounding**: Did retrieval provide usable evidence?
* **Confidence estimation**: Combined signal from retrieval + evaluation

### âœ… Refusal & Safety Logic

* Explicit refusal when confidence is low
* Avoids hallucination instead of masking it

### ðŸš€ Self-Improving Feedback Loop

* Automatic failure attribution:

  * weak retrieval
  * hallucination risk
  * low confidence
* Strategy retry (chunking / retrieval / prompt)
* Persistent memory of what worked

---

## ðŸ§  System Architecture

```
User Query
   â†“
Retriever (FAISS)
   â†“
Generator (GPU-backed LLM)
   â†“
Evaluation (Faithfulness, Grounding, Confidence)
   â†“
Failure Attribution
   â†“
Retry Policy (Strategy Switch)
   â†“
Memory Store (Learning)
```

This design mirrors **production LLM reliability pipelines**, not demos.

---

## ðŸ§ª Example Failure Handling

### Query

```
How does bad chunking affect gradient flow in transformer attention layers?
```

### System Behavior

* Retrieval finds chunking-related context
* Coverage is low (no mention of gradients or attention math)
* Faithfulness drops
* Confidence falls below threshold
* **System refuses instead of hallucinating**
* Retry attempts improve retrieval but still refuse if unsupported
* Failure and retry outcome are stored in memory

> Refusal is treated as a **successful outcome**, not a failure.

---

## ðŸ“ Project Structure

```text
self_improving_rag/
â”œâ”€â”€ ingestion/        # document loading & chunking
â”œâ”€â”€ retrieval/        # vector store & quality scoring
â”œâ”€â”€ generation/       # GPU-accelerated LLM inference
â”œâ”€â”€ evaluation/       # faithfulness, grounding, confidence
â”œâ”€â”€ feedback/         # failure attribution & retry logic
â”œâ”€â”€ memory/           # persistent learning store
â”œâ”€â”€ configs/          # YAML-based configuration
â”œâ”€â”€ docs/             # system & design documentation
â”œâ”€â”€ logs/             # ingestion & retrieval logs
â””â”€â”€ main_self_improving.py
```

---

## ðŸ“Š Evaluation Philosophy

### Why Accuracy Is Not Used

Accuracy requires ground-truth answers, which do not exist for:

* enterprise documents
* evolving policies
* ambiguous queries

### Metrics Used Instead

* **Faithfulness** â†’ answer tokens supported by context
* **Grounding** â†’ retrieval usefulness
* **Coverage** â†’ query intent captured
* **Confidence** â†’ weighted system trust score

This reflects how **real RAG systems are validated in production**.

---

## âš™ï¸ Performance & Deployment Notes

* 7B-parameter LLMs run locally using:

  * GPU + CPU offloading
  * 4-bit quantization
* HuggingFace Accelerate handles layer placement
* Some parameters may appear on the `meta` device â€” **expected behavior**
* System is designed to degrade safely under memory pressure

---

## ðŸ“Œ Known Trade-Offs

* Confidence thresholds are heuristic-based (policy, not training)
* Retry may marginally increase confidence without new information
* Future improvement: stricter refusal gating after retry

These are **documented design decisions**, not bugs.

---

## ðŸ”® Future Extensions

* Hybrid dense + sparse retrieval
* Clarifying question generation instead of refusal
* Automated evaluation dashboards
* Offline analysis of memory store for policy tuning

---

## ðŸ§‘â€ðŸ’¼ Intended Audience

This project is designed for:

* Applied GenAI Engineers
* LLM / RAG Reliability Engineers
* ML Engineers working on production LLM systems

It prioritizes **correctness, transparency, and safety** over flashy demos.

---

## ðŸ Final Note

> This system does not try to be clever.
> It tries to be **trustworthy**.



